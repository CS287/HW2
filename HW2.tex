\documentclass[11pt]{article}
\usepackage{common}
\title{CS 287 \\ Assignment 2: Tagging (almost) from Scratch }
\date{}
\begin{document}

\maketitle{}

In this homework assignment you will replicate current
research baseline results for text classification. Before 
starting the assignment be sure that you understand the model 
describe in the slides, in the text, and that you are familiar 
with the computational tutorial in HDF5 and Torch. 


You will be given a set of example sentences and labels. For instance: 


\vspace{0.5cm}

\noindent
\texttt{stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's " waking up in reno . " go back to sleep . }

You results will be evaluated based on the Kaggle competition dataset at \url{}.

\section{Preprocessing}

For this assignment we have provided a skeleton of preprocessing
code in \texttt{preprocessing.py}. Please read the code carefully.
For this assignment you will have to construct the training 
representations from the raw input data. You code should isolate 
the windows of the text and the corresponding output class (word tag).


[image]

In addition...

When done you should output the following matrices in an HDF5 group
\texttt{train\_input, train\_output, valid\_input, valid\_output,
  test\_input, nclasses, nfeatures, word2vec}.


\section{Classifier}

We will look for you code in \texttt{HW2.lua}, and it should be
written in Lua/Torch. For the first part of this assignment, you
should not use any non-debugging libraries besides the standard Torch
framework and HDF5. For the second part, you can use the \texttt{nn}
library.

\subsection{Prediction and Evaluation}

Be sure you are able to read and output the text data 
and that you understand the format. To confirm this, write a function 
that takes in the parameters of a linear model, and evaluates it on 
the validation set (\texttt{valid\_input} and \texttt{valid\_output}).
Report the results in terms of prediction accuracy. This same code should 
be used for each of the linear models described in this project. 

Also be sure you are able output your classification results on the test data as 
a text file. Check that you can upload these to the Kaggle.

\subsection{Hyperparameters}

Several of the models described have explicit hyperparameters that you will 
need to tune. It is your responsibility to cleanly separate these out from 
the models themselves and expose as command-line options. This makes it much 
easier to run experiments and to utilize experimental scripts. 

\subsection{Logging and Reporting}

As part of the write-up, you will need to report on the training and
predictive accuracy of your models. To make this possible, your code
should report on various metrics of the model both at training and
test time. We will leave it up to you on which metrics to log, but we
recommend reporting training speed, training set NLL, training set
predictive accuracy, and validation predictive accuracy. It is your
responsibility to convince us that the model is correctly training.

\section{Models}

For this assignment you will implement the three linear models that 
we discussed in class: naive bayes, multiclass logistic regression, 
and multiclass linear svm.

\subsection{Naive Bayes}

We recommend that you implement multinomial naive Bayes first, since
it is the most efficient and easiest to debug. Follow the description
given in the class notes and be sure to include the $\alpha$
hyperparameter.

\subsection{C\&W Model}

Next implement multiclass logistic regression with L2 regularization
(exposing the $\lambda$ hyperparameter). For efficiency, training
should be done using minibatch stochastic gradient descent. The size
of the minibatch, the learning rate, and the number of epochs of
training should also be exposed as hyperparameters.

Note that there are several tutorials for how to implement logistic
regression in Torch online. Do not use these. We expect the
implementation to only utilize the core tensor operations in the
framework.

Further notes:

\begin{itemize}
\item Bookmark \texttt{https://github.com/torch/nn} as that will be
  your main resource for this and future assignments. In particular
  you will want to read the documentation for \texttt{nn.LookupTable},
  \texttt{nn.Linear} (misnamed), \texttt{nn.ClassNLLCriterion}, and
  \texttt{nn.Tanh}.

\item 

\item We recommend starting with a subset of the full training data to
  verify that your code is working. You can confirm this by tracking
  the loss of the training set.
\end{itemize}

\subsection{C\&W With word2vec}

Finally implement hinge-loss with L2 regularization (exposing the $C$
hyperparameter). This code should use the same minibatch stochastic
gradient descent code as Logistic Regression. The only difference is
the calculation of the loss and the gradients. 

\subsection{} 

\subsection{Additional Experiments}

Once these models are constructed, you should also report on
additional experiments on these data sets. We will leave this aspect
open-ended, but suggestion include:

\begin{itemize}
\item Including additional features (for instance capitalization \cite{c&w})
\item Experimenting with different archetectures. 
\item Extensive further hyperparameter tuning and experimentation (for instance \cite{})
\item Running on specific data sets. 
\end{itemize}

\section{Report and Submission}

For your write-up, follow the report template at
\url{https://github.com/cs287/hw_template}. Be sure to include a link
to your code, Kaggle ID, and reports on your results.

In addition to submitting your Kaggle results, we also expect you to report on your 
experimental process. This should include data tables, graphs and discussion of any 
issues that you may run into. 


\end{document}
